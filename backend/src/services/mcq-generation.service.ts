/*
import { MCQModel } from '../models/mcq.model'

export class MCQGenerationService {
  private llmApiUrl = process.env.LLM_API_URL || 'http://localhost:11434'
  private modelName = process.env.LLM_MODEL_NAME || 'llama2'
  
  async generateMCQsForSegments(videoId: string, segments: any[]): Promise<any[]> {
    const allQuestions = []
    
    for (const segment of segments) {
      try {
        const questions = await this.generateMCQsForSegment(segment)
        
        // Save questions to database
        for (const question of questions) {
          const savedQuestion = await MCQModel.create({
            videoId,
            segmentId: segment._id,
            question: question.question,
            options: question.options,
            correctAnswer: question.correctAnswer,
            explanation: question.explanation
          })
          allQuestions.push(savedQuestion)
        }
      } catch (error) {
        console.error(`Failed to generate MCQs for segment ${segment.segmentNumber}:`, error)
      }
    }
    
    return allQuestions
  }
  
  private async generateMCQsForSegment(segment: any): Promise<any[]> {
    const prompt = this.createMCQPrompt(segment.text)
    
    try {
      // Call local LLM (Ollama) to generate MCQs
      const response = await fetch(`${this.llmApiUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: this.modelName,
          prompt: prompt,
          stream: false,
          options: {
            temperature: 0.7,
            max_tokens: 1000
          }
        })
      })
      
      if (!response.ok) {
        throw new Error(`LLM API error: ${response.statusText}`)
      }
      
      const result = await response.json()
      return this.parseMCQResponse(result.response)
    } catch (error) {
      console.error('LLM generation failed:', error)
      throw error
    }
  }
  
  private createMCQPrompt(transcriptText: string): string {
    return `
Based on the following transcript segment, generate 2-3 multiple choice questions that test comprehension and key concepts. 

Transcript:
"${transcriptText}"

Please format your response as JSON with the following structure:
{
  "questions": [
    {
      "question": "Question text here?",
      "options": ["Option A", "Option B", "Option C", "Option D"],
      "correctAnswer": 0,
      "explanation": "Explanation of why this answer is correct"
    }
  ]
}

Guidelines:
- Focus on key concepts, facts, and important information
- Make questions clear and unambiguous
- Ensure options are plausible but only one is clearly correct
- Provide helpful explanations
- Avoid trivial or overly complex questions
- Generate 2-3 questions per segment

Response:
`
  }
  
  private parseMCQResponse(response: string): any[] {
    try {
      // Clean up the response (remove any markdown formatting)
      const cleanResponse = response
        .replace(/```json/g, '')
        .replace(/```/g, '')
        .trim()
      
      const parsed = JSON.parse(cleanResponse)
      
      if (parsed.questions && Array.isArray(parsed.questions)) {
        return parsed.questions.map(q => ({
          question: q.question,
          options: q.options || [],
          correctAnswer: q.correctAnswer || 0,
          explanation: q.explanation || ''
        }))
      }
      
      throw new Error('Invalid response format')
    } catch (error) {
      console.error('Failed to parse MCQ response:', error)
      
      // Fallback: try to extract questions manually
      return this.fallbackParsing(response)
    }
  }
  
  private fallbackParsing(response: string): any[] {
    // Simple fallback parsing if JSON parsing fails
    // This is a basic implementation - you might want to make it more robust
    const questions = []
    const lines = response.split('\n')
    
    let currentQuestion = null
    let currentOptions = []
    
    for (const line of lines) {
      const trimmed = line.trim()
      
      if (trimmed.includes('?')) {
        if (currentQuestion) {
          questions.push({
            question: currentQuestion,
            options: currentOptions,
            correctAnswer: 0,
            explanation: 'Generated by AI'
          })
        }
        currentQuestion = trimmed
        currentOptions = []
      } else if (trimmed.match(/^[A-D][\.\)]/)) {
        currentOptions.push(trimmed.substring(2).trim())
      }
    }
    
    if (currentQuestion && currentOptions.length > 0) {
      questions.push({
        question: currentQuestion,
        options: currentOptions,
        correctAnswer: 0,
        explanation: 'Generated by AI'
      })
    }
    
    return questions
  }
  
  async getQuestions(videoId: string): Promise<any[]> {
    return await MCQModel.find({ videoId }).populate('segmentId')
  }
  
  async updateQuestion(questionId: string, updates: any): Promise<any> {
    return await MCQModel.findByIdAndUpdate(questionId, updates, { new: true })
  }
  
  async deleteQuestion(questionId: string): Promise<void> {
    await MCQModel.findByIdAndDelete(questionId)
  }
}

// Alternative implementation for different LLM providers
export class OpenAIService {
  async generateMCQs(transcriptText: string): Promise<any[]> {
    // If using OpenAI API instead of local LLM
    // const response = await fetch('https://api.openai.com/v1/chat/completions', {
    //   method: 'POST',
    //   headers: {
    //     'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
    //     'Content-Type': 'application/json'
    //   },
    //   body: JSON.stringify({
    //     model: 'gpt-3.5-turbo',
    //     messages: [
    //       {
    //         role: 'system',
    //         content: 'You are an expert educator who creates high-quality multiple choice questions.'
    //       },
    //       {
    //         role: 'user',
    //         content: this.createMCQPrompt(transcriptText)
    //       }
    //     ],
    //     temperature: 0.7
    //   })
    // })
    
    throw new Error('Implement OpenAI integration')
  }
}
*/

// TODO: Uncomment and implement the above code
// This service handles:
// 1. MCQ generation using local LLM (Ollama/LLaMA)
// 2. Prompt engineering for quality questions
// 3. Response parsing and validation
// 4. Database storage of generated questions
// 5. Question management (update, delete)

export {}
